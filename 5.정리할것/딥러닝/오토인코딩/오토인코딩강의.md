# 오토인코딩 강의 정리
DIP 현장실습 도중 오토인코딩을 도로 아스팔트 손상을 구별하는데 활용하기 위해 학습함

### 오토인코더 구조도
![AE01](./AE01.png)

## 기본적인 키워드
- Unsupervised learning
- Manifold learning, Nonlinear Dimensional reduction
- Generative model learning
- ML(Maximul likelyhood) density estimate

## 1. 딥러닝 기본 정리
모두가 딥러닝을 활용하지만 정확히 기본 개념을 모름. 이번 기회에 정리를 해보자.

딥뉴럴네트워크를 할때 ML estimate를 하는거다라는걸 이해하면 성공

### 전통적인 머신 러닝 접근 방식

![AE02](./AE02.png)

- 데이터에서 정보를 얻을려고 함 -> 데이터를 모음
- 모델을 정함. 모델 => f
- 모델을 결정짓는 파라미터를 추정해야됨 <= 학습
- 파라미터에 대해 입략값의 출력값이 실제추출되야하는 데이터와 다른정도 => loss function : 이것도 정하고 감.
- 모델을 결정짓는 파라미터를 변화시키면서 로스를 가장 최소로 만듬 (학습)
- 입력값이 고정되면 고정된 출력값이 나옴

### 그래서 고전적 해석으로 딥러닝 어떻게 해석?
- 모델을 딥뉴럴넷으로 정함. 네트워크 구조 정의 등등
- loss function 정의해야됨. 현제 딥러닝은 MSE, cross entropy  아무거나 못 쓰기 때문에(back propagation 알고리즘을 통해서 학습을 하기 때문에 제약 조건이 생김)
- back propagation 알고리즘 loss function에 대한 가정 2개가 필요 1. 전체 로스는 각 로스의 합 2. 로스는 출력값만 가지고 구한다.

gradient Descent 방법에서의 파라미터 변경법은 아래와 같음

![AE03](./AE03.png)

그 뒤에는 전체에 대한 gradient를 구해야되는데 너무 양이 많아서 배치 한개의 평균을 구해서 구함. 

한번 평균 구하는것 = 스탭

전체 다한거 = epoch

원래 각 레이어별 많은 파라미터의 로스 함수를 미분해서 구했었는데 그 연산량이 많아서 back propagation 알고리즘 나오기 전에 암흑기 였음

### - backpropagation 관점
MSE :액티베이션 함수 미분값이 0에 가까우면 웨이트와 바이어스 업데이트가 더디다. grandient vanishing problem 그럼 미분값이 이쁘게 들어가게 하면됨 -> relu

cross entropy: 액티베이션 함수 미분값이 공교롭게 필요없다. 그래서 미분값에 무관하게 된다. 학습에 강인함

이관점에서는 cross entropy가 더 좋은듯함. 물론 항상은 아님

### - ML 관점
이 관점에서는 네트워크 출력값에 대한 해석이 중요함

네트워크 출력값이 y가 나올 확률이 최대가 되게하고싶다.
그래서 그 확률들의 분포도 정하고간다.

네트워크 출력은 확률 분포를 정의하는 파라미터를 추정하는것
-> 평균이랑 표준 편차


## 2. Manifold learning