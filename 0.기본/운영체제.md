# 운영체제

* [개요](#개요)
* [운영체제 구조](#운영체제-구조)
* [프로세스](#프로세스)
* [스레드](#스레드)
* [프로세스 스케줄링](#프로세스-스케줄링)
* [동기](#동기)
* [데드락](#데드락)
* [메모리 관리](#메모리-관리)
* [가상 메모리 관리](#가상-메모리-관리) 

---
</br>

## 개요

운영체제란 `하드웨어를 관리하고, 응용 프로그램과 하드웨어 사이에서 인터페이스 역활을 하며 시스템의 동작을 제어하는 시스템 소프트웨어로 정의`한다.

즉 시스템의 자원과 동작을 관리하는 소프트웨어이다. 또한 사용자의 편의성을 높여준다.

### 컴퓨터 시스템 조직

* CPU: 계산을 실행
* I/O device: 디스크, 모니터, 프린터 등등
* 메모리: 프로그램과 데이터 저장
* 시스템 버스: 통신 채널

### 컴퓨터 부팅

* bootstrap program: 컴퓨터가 켜졌을때 가장먼저 켜지는 프로그램, ROM이나 EEPROM(firmware)에 저장되있음. CPU 레지스터나 디바이스 컨트롤러, 메모리를 초기화한다.
* 시스템 `데몬`: 백그라운드에서 커널이 돌아가는 전체 시간동안 켜져있다. `init`이 첫 프로세스이며 다른 `데몬`들을 시작시킨다.

### Firmware?

간단하게 하드웨어의 제어와 구동을 담당하는 프로그램

### 스토리지 구조

* 프로그램은 무조건 메모리에 로드된다.
* 폰노이만 구조
  * 인스트럭션과 데이터는 같은 메모리에 저장된다
  * CPU는 다양한 기능을 하는 장치이다. 메모리는 캐쉬를 사용하여 향상될수있다.

* 보조 저장장치: 메인 메모리가 작고 휘발성이 있어서 존재
* 저장 장치 계층(Storage device hierarchy)
* 비휘발 저장장치
  * SSD
  * flash memory
  * NVRAM: 배터리 보조 전원있는 RAM

### I/O 구조

* 디바이스 컨트롤러
  * CPU와 디바이스 컨트롤러가 버스로 연결된다. ex) SCSI 컨트롤러
  * 자체적인 버퍼와 레지스터 존재한다.
  * 데이터 인/아웃을 책임진다.
  * OS는 디바이스 드라이버라는 디바이스 컨트롤러와 상호작용하는 것을 가지고 있다.
* I/O 작업
    1. 디바이스 드라이버가 디바이스 컨트롤러의 레지스터를 로드한다.
    2. 디바이스 컨트롤러는 레지스터 내용을 검사한다.
    3. 디바이스 컨트롤러가 데이터를 전송한다.
    4. 끝나면 컨트롤러가 인터럽트를 건다
    5. 드라이버가 운체에게 조절권을 돌려준다.
* DMA

### 컴퓨터 시스템 구조

* 단일 프로세서 시스템(single-processor)
  * 한개의 다목적 프로세서와 다양한 특수한 프로세서
* 다중 프로세서 시스템(multi-processor)
  * 왜 다중? 단일 프로세서로는 퍼포먼스를 더이상 향상 못시킴
  * 다중 프로세서의 장점
    * throughput 증가
* 비동기 멀티프로세싱: master와 slave

* 동기 멀티프로세싱(SMP)
  * 모든 프로세서가 peers
  * 모두 메모리를 공유한다.
* 멀티코어 시스템

## 운영체제 구조

### OS 인터페이스

* CLI
* GUI

### 시스템 콜

* 시스템 콜은 간단한 작업이라도 많이 쓰인다.
* 프로그램은 간접적으로도 시스템콜을 사용한다.
* 그럼 왜 API 사용?
  * 시스템 콜 바로 사용하면 위험
  * 이식성

### 시스템콜 인터페이스

시스템콜과 연결된 함수 라이브러리의 집합

### 시스템콜 종류

* 프로세스 관리
* 파일 조작
* 디바이스 조작
* 정보 관리
* 통신
* 보호

### 운영체제 디자인과 구현

* 디자인 목표
  * 유저: 사용 편의, 신뢰성, 속도
  * 시스템: 구현, 관리, 동작의 편의, 효율성, 유연성, fault-tolerant
* 가이드라인: 메커니즘과 정책
  * 메커니즘은 어떻게, 정책은 무엇을
    ex) timer 메커니즘, 스케쥴링 정책
  * 왜 분리함? 정책은 바뀜, 정책이 바뀐다고 메커니즘도 바뀌는건 좀 아니지 않나?

### OS 구현

* 어셈블리어: 복잡하다..., 어셈블리어는 CPU마다 다르기때문에 다른 PC에서 돌릴려면 에뮬레이터 필요
* 고급 언어: 구현이 편한고 컴파일러가 좋아져서 좋아짐, 에뮬레이터 필요없음, 성능이 좀 안좋음 -> 그래서 일부는 최적화시킴

### OS 구조

* 레이어 구조
* 마이크로커널

## 프로세스

### 프로세스(Process)

* 실행중인 프로그램으로 디스크로부터 메모리에 적재되어 CPU를 할당 받을 수 있는거
* 운체로부터 주소 공간, 파일, 메모리 등을 할당 받으며 이것들을 프로세스라고 한다.
* 프로세스 스택과 전역 변수를 수록하는 데이터 섹션을 포함한다. 또한 동적으로 할당되는 메모리인 힙을 포함한다.

### 프로세스 메모리

* 스택: 함수호출시 아래로 확장됨
* 힙: 동적 할당시 위로 확장됨
* 데이터: 전역 변수
* 텍스트: 이진 프로그램 코드

공유하여 메모리 사용량을 줄임 -> Code는 같은 프로그램 자체에서는 모두 같은 내용, Stack과 data는 스택 구조의 특성과 전역 변수의 활용성 때문

### 프로세스 상태

* new
* running: 실행되는중, 한개의 프로세서에서는 한개만
* waiting: 이벤트 기다리는중
* ready: 프로세서 할당대기중
* terminated: 종료됨

### PCB(프로세스 제어 블록)

* 특정 프로세스에 대한 중요한 정보를 저장하고 있는 것
* 프로세스의 생성과 동시에 고유한 PCB를 생성한다.
* CPU를 할당받아 작업을 처리하다 프로세스 전환이 발생하면 진행하던 작업을 저장하고 CPU를 반환해야하는데 이때 작업의 진행사항을 PCB에 저장한다.
* 다시 CPU를 할당받으면 PCB에 저장된 내용을 불러온다.

### PCB에 저장되는 정보

* 프로세스 식별자(PID): 프로세스 식별번호
* 프로세스 식별자(Process ID, PID) : 프로세스 식별번호
* 프로세스 상태 : new, ready, running, waiting, terminated 등의 상태를 저장
* 프로그램 카운터 : 프로세스가 다음에 실행할 명령어의 주소
* CPU 레지스터
* CPU 스케쥴링 정보 : 프로세스의 우선순위, 스케줄 큐에 대한 포인터 등
* 메모리 관리 정보 : 페이지 테이블 또는 세그먼트 테이블 등과 같은 정보를 포함
* 입출력 상태 정보 : 프로세스에 할당된 입출력 장치들과 열린 파일 목록
* 어카운팅 정보 : 사용된 CPU 시간, 시간제한, 계정번호 등

### 스레드(Thread)

* 스레드는 프로세스의 실행단위
* 프로세스 내에서 동작되는 여러 실행 흐름, 주소 공간이나 자원을 공유한다.
* 스레드는 스레드 ID, 프로그램 카운터, 레지스터 집합, 그리고 스택으로 구성된다.
* 같은 프로세스에 속한 다른 쓰레드와 코드, 데이터, 열린 파일이나 신호와 같은 운영체제 자원들을 공유한다.
* 하나의 프로세스를 다수의 실행단위로 구분하여 자원을 공유하고 자원의 생성과 관리의 중복성을 최소화하여 수행 능력을 향상시키는 것을 멀티스레딩이라고한다. 이 경우 각각의 스레드는 독립적인 작업을 수행해야하기 때문에 각자의 스택과 PC 레지스터 값을 갖고 있다.

### 프로세스 스케쥴링

* 멀티프로그래밍: CPU 활용도 최대로
* 시간 공유: 프로세스가 상호작용할 수 있도록

-> 프로세스 스케줄링

* 스케줄링 큐
  * job 큐: 모든 프로세스
  * ready 큐: 프로세서에서 작동되기를 기다리는 프로세스들, PCB의 연결리스트로 구현
  * device 큐: I/O 장치를 기다리는 프로세스의 리스트, 매장치마다 큐가 있음

### 스케줄러

* 큐에서 정책에 따라 프로세스를 선택해줌
* 장기 스케줄러: job 스케줄러
* 단기 스케줄러: CPU 스케줄러
* 장기 스케줄러는 I/O랑 CPU 바운드 프로세스를 잘조절해야한다
* 기준: CPU 활용도, 대기 시간, 응답시간, 공정성

### 인터럽트

* 프로그램을 실행하는 도중에 예기치 않을 상황이 발생할 경우 실행 중인 작업을 중단하고, 상황부터 처리가 필요하다고 CPU에게 알리는것
* 입출력, 우선 순위 연산 같은 일이...
* 외부/내부 인터럽트는 CPU의 하드웨어 신호(외부:전원 이상, 기계 착오, 외부 신호, 입출력 내부: Trap이라고 부르며, 잘못된 명령이나 데이터를 사용할때 발생)에 의해 발생
* 소프트웨어 인터럽트는 명령어의 수행에 의해 발생 (SVC 인터럽트)

### Context Switch?

* 프로세스의 상태 정보를 저장하고 복원하는 과정
* 프로세스는 각 독립된 메모리 영역을 할당받아 사용하므로 캐시 메모리 초기화로 오버헤드가 발생할 문제 존재 -> 줄이려면 콘텍스트 정보를 줄이면됨
* 하드웨어가 시간을 줄일수있다. 레지스터를 많이 두거나, 간단히 포인터를 바꿀수있는 등등

### 프로세스 종료

* exit()을 만나면 종료한다.
* 부모가 자식을 종료할수있다
* 좀비: 종료됬지만 부모 프로세스가 wait를 콜안한 경우 리턴에서 멈춰있다.
* 만약 절대 wait를 콜안하면? init이 받아서 wait를 콜해준다.

### IPC

* 프로세스간에 데이터 교환
* 공유메모리
* 메세지 전송

### 공유 메모리

* 프로세스가 주소공간에 메모리 구역을 준비하고
* 다른 프로세스의 주소공간에 그 공간을 붙인다.
* OS가 메모리 접근 제한을 풀어줘야된다.
* producer-consumer의 개념

### 메세지 전달 방식

* 직접 전송
* 간접 전송
* 왜 두개?
  * 직접 통신의 경우 컴파일 할때 이름을 알수없거나 상대방의 이름이 달라지면 문제가 생긴다.
  * 그래서 메일 박스(간접 전송) 등장
  * 메일 박슨는 동기화 문제가 생김
* synchronization: 동기 중간중간에 확인하는거
* asyncho..: 비동기 결과만 받음
* blocking: 상대가 끝날때까지 기다리고만 있음
* nonblocking: 나도 일할수있어

### 소켓

* 통신을 위한 endpoint
* IP+port
* 서버Ip+port, 크라이언트IP+port, 프로토콜로 통신함

### 파이프

* 파이프로 통신가능, 단방향임
* Named pipe
  * FIFO라고도 함
  * 자식, 부모 관계없음
  * 여러 프로세스가 사용가능
  * 프로세스의 생명주기에 안묶임
  * mkfifo()

## 스레드

### 스레드 VS 프로세스

* 프로세스는 자신만의 고유 공간과 자원을 할당받아 사용
* 스레드는 다른 스레드와 공간 자원을 공유하면서 사용
* 왜 사용?
  * 더 좋은 성능으로 병렬성을 높인다
  * 자원 사용에 효율성
  * 데이터 공유 쉽다

### 멀티코어 프로그래밍

* 멀티코어 프로세서:한칩에 여러 CPU단위가 들어있다.
* 쓰레드에 영향: 병렬 실행에 더욱 효율적

### concurrency vs parallelism

* parallelism: 동시에 여러 작업수행
* concurrency: 진행도중에 여러 작업을 수행

### 멀티코어가 해결해야할점

* 태스크 구별
* 밸런스
* 데이터 나누기
* 데이터 의존
* 테스트와 디버그

### parallelism의 종류

* Data: 데이터를 나눔
* task: 작업을 나눔
* 실제로는 둘다 사용

### 멀티 프로세스

* 여려 프로세스를 두고 동시에 처리함
* 장점: 안정성(메모리 침범 문제를 OS차원에서 해결)
* 단점: 각각 독립된 메모리 영역을 갖고있어, 작업량이 많을 수록 오버헤드 발생, Context Switshing으로 인한 성능 저하

### 멀티스레드

* 프로세스를 이용하여 동시에 처리하던 일을 스레드로 구현
* 메모리 공간과 시스템 자원 소모가 감소
* 스레드간 통신이 필요한 경우에도 별도 자원 필요 없음
* 스레드의 context switch는 캐시 메모리를 비울 필요가 없기 때문에 더 빠르다.
* 시스템의 throughput(처리율)이 향상되고 자원 소모가 줄어들며 응답시간이 단축된다.

#### 단점

* 동일한 자원을 여러 스레드가 공유하기 때문에 엉뚱한 값을 불러오거나 저장하는 문제가 있다.
* 그래서 동기와 작업이 필요, 동기화를 통해 작업 처리 순서를 컨트롤, 그러나 병목현상이 발생할 수 있다.
* 안정성 문제, 하나의 스레드가 데이터 공간을 망가뜨리면 모든 스레드가 작동불능
* Critical Section(상호 배제, 진행, 한정된 대기를 충족) 기법으로 해결가능

### 유저 vs 커널 스레드

* 유저 스레드: 유저 레벨 라이브러리 사용,OS커널은 얼마나 많은 스레드가 있는지 모름
  * 장점: 빠르다, 시스템 콜 필요없음
  * 단점: 커널의 관리를 안받음, 스케줄링이 안됨
* 커널 스레드: 스케줄 가능한 대상
  * 장점: 스케줄대상이라 유연함
  * 단점: 느리다

* Nto1 모델: 1개의 커널 스레드에 여러 유저스레드
  * 장: 빠름, 이식성
  * 단: 병렬실행 안됨, 프로세스가 block되면 스레드도 block
* 1-1모델: 1개의 커널 스레드에 1개의 커널 스레드
  * 장: 동시성이 올라감,멀티코어 CPU를 더 잘사용함
  * 단: 커널작업, 커널 자원 소모, 개수제한
* NtoN 모델: 여러개 여러개
  * 유연함
  * 복잡함

### Pthread

* POSIX 표준(IEEE 1003.1c)
* Pthread API

### 자바 스레드

* 스레드 클래스를 상속받아서 run() 오버라이딩하고 메소드 실행
* runnable interface를 implement해서 만든다

### 스레드 풀

* 스레드 만들고 삭제하는게 오버헤드다
* 스레드 생성이 무한정이다
* 스레드 풀: 고정된 수의 스레드를 먼저 만들고, 요청이 들어오면 스레드 풀의 스레드를 주고, 리턴시 풀로 돌려준다.

### 스레드 이슈

* fork() exec()
  * fork()시 모든 스레드가 복사되거나 한개만 복사둘다 된다.
  * 만약 exec()이 호출되면 호출스레드 말고 다 죽임
* 시그널 처리
  * 시그널이 적용되는 스레드만
  * 모두
  * 특정 스레드만
  * 특별한 시그널 처리 스레드
* 스레드 종료
    1. Async cancellation : thread가 다른 하나의 thread를 선택해서 사살
    2. Deferred cancellation : 내가 죽어야 하나 계속 확인(periodically)
* Deferred cancellation이 종료 작업을 할기회가 주어짐

## 프로세스 스케줄링

* CPU 스케줄링: 프로세스간에 CPU 변환
* 실제 스케줄링 단위는 스레드임
* CPU 스케줄링, 스레드 스케줄링, 프로세스 스케줄링 모두 비슷한 단어
* 왜 스케줄링이 필요한가?
  * 프로세서의 효율성을 높이고, 프로세스의 응답시간을 최소화

### CPU-I/O Burst Cycle

* 프로세스 실행은 CPU실행과 I/O 대기의 사이클이다.
* CPU Burst duration

### CPU 스케줄러(단기 스케줄러)

* CPU가 idle 상태가 되면 OS가 ready 큐에서 프로세스를 선택함
* 스케줄러가 선택을 할때
    1. 프로세스가 running 상태에서 waiting 상태로 변환(I/O 요청, wait() 발동)
    2. 프로세스가 running 상태에서 ready 상태로 변환(인터럽트 발생)
    3. 프로세스가 waiting state에서 ready 상태로 변환
    4. 프로세스가 종료

### 선점식 vs 비선점식

* 비선점식 스케줄러: 1과 4 경우만 한다
* 돌고있는 프로세스를 리스케줄할수있는가?
  * 선점식:yes
  * 비선점: no
* 비선점인경우 프로세스가 자발적으로 CPU를 놓거나 끝나야된다.

### 선점식 스케줄링

* 문제
  * race 컨디션의 원인
    * 한프로세스가 공유된 데이터에 접근할때 선점이 일어난다.
    * 다른 프로세스는 내용이 다른 데이터를 읽는다
  * 커널 디자인이 복잡해짐
    * 시스템콜 다루는 도중에 선점이 일어나면
    * 시스템콜 도중에는 선점 방지
    * 실시간 컴퓨팅에는 안좋다.

### 디스패처

* 선택된 프로세스에게 CPU의 제어권을 주는 요소
* 맡은 일
  * 컨택스트 변환
  * 유저모드로 변환
  * 프로그램 재시작을 위해 뛰어넘을때

* 디스패치 latency: 한 프로세스를 멈추고 다른 프로세스를 시작시키는 시간

### 스케줄링 기준

* CPU utilization
* throughput
* turnaround time
* waiting time
* response time
* fairness

### FCFS 스케줄링

* First-come, first-served 스케줄링
  * 비선점
  * 구현하기 간단함

### SJF shortest job first 스케줄링

* 짧은거부터
* 최적 최소 대기시간
* CPU Burst 시간을 알기 힘들다. 즉 실행 시간을 미리 알기 힘들다.
* 장기 스케줄링에 자주 사용됨

* 선점 방식으로는 SRTF

### 우선순위 스케줄링

* 두가지의 우선순위가 있다.
  * 내부: 우선순위 계산을 위한 척도가 있다
  * 외부: 운영체제 밖의 기준
* 선점 비선점 둘다 가능

### 기아 문제

* 우선순위 스케줄링의 문제점
* 저 우선순위 프로세스는 스케줄될 기회도 없다.
* aging

### Round-Robin 스케줄링

* time-sharing system(시분활)
* FCFS 랑 비슷한데 선점이다
* 시간 퀀텀으로 나뉘다
* 준비 큐는 FIFO, 새로운 프로세스는 꼬리에 넣는다.

### 멀티레벨 큐 스케줄링

* 다른 군집으로 분류된 프로세스들을 관리하기 위해서
  * 각 클래스는 다른 요구사항
  * 시스템 VS 반응형 VS 배치
* 레디 큐를 여러 큐로 나눈다

* 멀티 레벨큐
  * CPU를 어느 큐에 할당할지
  * 우선순위가 높은 큐가 계속 차있으면 밑에는 CPU를 못받음

### 멀티레벨 피드백 큐 스케줄링

* 그래서 멀티레벨 피드백 큐
  * 짧은 거 부터 실행해서 대기 시간을 줄이고
  * 대회형 사용자에게 빠른 응답시간을 주고 싶다.
  * 긴 프로세스는 아래로 내려간다.
  * 대화형과 I/O프로세서에 좋다.
  * 기아문제는 에이징으로 해결

## 동기

### 동기화의 필요성

* 멀티 스레드 프로그램
  * 스레드는 프로세스안에서 메모리를 공유한다.
  * 공유 변수를 관리를 안해주면 안된다
  * 동기화가 silent corruption을 방지하기 위해서는 필요하다.
* 커널 모드
  * 커널 데이터 구조가 업데이트 될때
  * 선점식 커널 VS 비선점식 커널
  * 비선점은 레이스 컨디션없다.
    대신 구현이 어렵다.
  * 그럼 왜 선점 커널을 사용하는가?
    * 비선점형에서 한 프로세스가 뻑나가면 컴터를 껐다 켜야됨
    * 응답성 때문에 선점형 커널이 사용됨

### 크리티컬 섹션 문제

* 크리티컬 섹션이란
  * 공통의 변수를 수정하는 코드 부분
* 크리티컬 섹션 문제: 단 한개의 프로세스만 크리티컬 섹션에 진입하도록 하는 프로토콜 제작
* 해결법은 3가지 조건을 만족해야함
  * 상호 배제(mutual exclusive): 한 프로세스만 CS에 들어가야됨
  * 프로그레스: CS에 프로세스가 없으면 리메인더 섹션에 없는 P는 즉시 들어가야된다.
  * 바운디드 웨이트: CS에 들어가겠다고 요청한 다음 한정되 시간만큼 기다려야한다.

### 피터슨 알고리즘

* 상호배제: 동시에 들어갈수없다.
* progress: flag 때문에 CS비면 바로 들어간다.
* bounded: turn 때문에 무조건 한번은 할수있다.

### 하드웨어적인 지원

* 싱글코어에서 인터럽트를 못하게한다.
  * 멀티코어에서는 모든 코어의 인터럽트가 꺼지므로 더 오래걸린다.
  * 인터럽트를 마음데로 끄면 문제가 생길 수 있다.
* 메모리 블록
* 특별한 하드웨어 인스트럭션이 있다.
  test_and_set(), compare_and_swap()

### 뮤텍스 락

* 하드웨어 락은 복잡해서 응용 프로그래머가 사용하기 힘들다.
* OS 디자이너는 CS문제를 위해 SW도구를 만들어 놨
다.
  * Mutex : mutual exclusion
  * 락킹 해결법
* 비지 웨이팅: 루프에서 기다리는거
  * CPU낭비
  * 짧은 락이면 좋다. 컨텍스트 스위치가 없어서

### 세마포어

* wait() and signal()

### 세마포어 사용

* 카운트 세마포어: 자원이 유한할때 사용가능
* 실행 순서를 조절할때

### 세마포어 구현

* 세마포어도 비지 웨이팅

* 블록킹을 사용해서 방지하자
  * wait()에서 프로세스를 waiting 큐에 넣는다.
    * wait state로 만든다.
    * CPU 스케줄러는 다른 프로세스 돌린다.
  * signal()에서
    * wakeup()으로 깨운다.: wait queue에서 레디큐로

### 데드락과 기아

* 데드락도 발생할 수 있다.
* 기아도 발생가능 wait 큐에있으면

## 데드락

### 데드락의 정의

* 시스템 모델
  * 유한한 자원
  * 자원은 몇가지로 나누어 져있다.
  * 프로세스는 사용전에 자원을 요청하고 다 사용하면 풀어준다.
* 정의: 모든 프로세스가 다른 프로세스에의해 발생하는 이벤트를 기다리고 있는 상태

### 데드락의 조건

* 필수조건
  * Mutural exclusion: 한개의 리소스는 공유못한다.
  * Hold and wait: 한개의 리소스를 가지고 다른 프로세스가 들고있는 자원이 필요하다면 기다리고 있다.
  * No preemption: 자원은 자발적으로만 풀수있다.
  * circular wait: 순환으로 기다리는것

### Resource Allocation Graph

* 방향 그래프
* V E의 집합
* V는 자원과 프로세스로 나뉜다.
* edges

### 데드락: 사이클이 있다면 데드락 존재할수있다

* 한개의 인스턴스만 있으면 데드락
* 여러개면 무조건은 아니다.

### 데드락 다루기

* 3가지 전략
  * 막거나 회피
  * 데드락 들어가게 두고, 감지해서 복구하자
  * 무시하자: 예방, 회피는 그럴만한 가치가 없을 수도 있다. 잘일어나지도 않는다.

### 데드락 예방

* 4가지 조건중 한개이상을 부정
  * mutual exclusion: 자원을 공유하게 하자, 근데 힘들다.
  * Hold and Wait: 모든 리소스를 한꺼번에 요청을 하도록 하자-> 자원 관리 비효율, 자원이 없을 때만 요청시키자 -> 기아 문제
  * 비선점: 선점되게 하자, 자원이 선점이 가능할때만 된다.
  * 환형대기: 자원의 순서를 매기고 순서에 따라 요구한다.

### 데드락 회피

* 어떻게 자원이 요청되는지에 대한 정보가 필요하다.
* 각 요청에 대해 기다려야는지 아닌지에 대해 결정한다.
* 가장 간단한 방법: 자원의 최대갯수

* safe state: 시스템은 마음대로 할당할수있지만 데드락은 발생안한다.
* unsafe state: 모든 상태가 데드락인건 아니지만, 데드락을 이끌수있다.

* 데드락 회피 알고리즘의 핵심
  * 시작점은 safe
  * 시스템은 바로 자원을 할당할지 안할찌 선택한다.
  * 할당이 safe 상태에 머물때만 승인한다.

### 리소스 할당 그래프 알고리즘

* 사이클이 형성 안될때 승인해줌

### 뱅커스 알고리즘

* 리소스 할당 그래프는 각 자원마다 할당된 인스턴스가 한개일때만 가능하다.
* 뱅커스 알고리즘
  * 새 프로세스가 들어오면, 각 리소스의 최대값을 선언해준다.
  * 요청이 들어오면, 요청대로 할당할껀지 아닐지 선택한다.
* 안정상태 확인: 특정 순서대로 했을때 자원이 모자르지 않고 되면 안정
* 할당 해주고 안정상태이면 할당 해준다.

## 메모리 관리

### 메모리 접근

* 프로세스는 실행될려면 메모리에 불러와져야됨
* CPU는 레지스터와 메인 메모리만 접근 가능하다.
* 메모리 유닛은 메모리 주소의 스트림이다.
* 메모리 보호: limit 레지스터와 base레지스터로 범위 한정

### 주소 할당

* 프로그램 라이프사이클
  * 프로그램 실행할수있는 바이너리는 디스크에 있다
  * 프로그램은 실행가는한 형태로 메모리로 가져와지는 것을 input 큐에서 기다림
  * 인풋 큐에서 선택된다.
  * 메모리로 로드된다. 인스트럭션과 데이터를 실행동안 접근가능
  * 끝나면 메모리 풀어준다.
* 현대적인 OS에서는 프로세스가 물리주소의 일부분에 존재한다.
* 컴파일러는 심볼릭 주소를 변경가능한 주소에 할당한다.
* 링커와 로더는 변경가능한 주소를 절대 주소에 할당한다.

### 주소 할당 시간

* 3가지의 다른 스테이지
  * 컴파일타임: 절대 코드가 생성됨
  * 로드 타임: 변경가능한 주소가 생성됨
  * 실행 타임: 한 세그먼트에서 다른 쪽으로 이동할때까지 딜레이된다.

### 논리적 VS 물리적 주소 공간

* 물리적 주소: 메모리 유닛에서 볼수있는 주소
* 논리적 주소: CPU에 의해서 생성되는 주소, 가상 주소라고도 부름
* 논리적 물리적 주소는 컴파일 시간과 로드 시간에는 같다.
* 실행시간에는 다르다.
* 논리적 주소 공간: 모든 논리적 주소의 집합
* 물리적 주소 공간: 모든 물리적인 주소의 집합인다. 모두 논리적 주소에 대응된다.

### MMU(Memory management unit)

* 논리적인 주소를 물리적인 주소로 매핑해준다.
* 유저 프로그램은 절대 물리적인 주소를 보지 못한다.
* 프로그램은 논리적 주소로 조작하고 해결한다.
* 물리적인 주소는 접근할때만 결정됨

### 동적 로딩

* 메모리 크기를 초과하는 프로그램을 로드하면 우짬?
* 적용되기 전까지는 루틴이 로드 되지 않는다.
* 모든 루틴은 디스크에 재배치가능한 형태로 유지되어진다.
* 메인 함수의 실행으로 시작됨
* 루틴을 불러올때 이미 로드 됬는지 확인하고 재배치가능한 링크 로더가 로드하고 주소 테이블에 추가한다.
* 장점: 사용하지 않는 루틴은 절대 로드 안됨

### 동적 링킹

* 정적 VS 동적
  * 정적: 로더에 의해 이진 프로그램이미지로 시스템 라이브러리를 바꾼다.
  * 동적: 실행 시간까지 링크를 연기한다.
* 동적 링킹
  * 코드의 작은 조각(스텁)은 라이브러리 루틴이 언급된 바이너리에 포함된다.
  * 스텁은 메모리에 상주하는 라이브러리 루틴을 어떻게 찾는지 알고있다.
  * 스텁은 루틴의 주소로 교체한다.
* 라이브러리 업데이트
  * 라이브러리가 업데이트될때 동적 링킹이 다른 프로그램 전체와 링크거는것을 피하게 해준다.
  * 버전 정보가 프로그램과 라이브러리에 들었다.
  * 프로그램은 적절한 버전을 사용한다.
* OS는 동적 링킹이 필요하다. 다른 주소공간도 확인해야하고 접근을 해야한다.
* 동적 링킹은 느리다. 주소를 따라가야 해서
* 불일치 문제도 고려해야함

### 스와핑

* 전체 메모리 공간이 물리적인 공간을 초과하는 프로세스를 돌려야하는데 스와핑이 사용된다.
* 스와핑은 일시적으로 저장했다가 가져오는 일을 한다.
* 롤아웃, 롤인: 우선순위 스케줄러에서 사용됨 저 우선순위 프로세스는 스왑아웃함으로 우선순위가 높은 친구를 돌릴수있다.
* 어디서 스왑되냐?
  * 컴파일, 로드 시간: 무조건 이전 주소로 로드 되야됨
  * 실행시간: 다른 메모리공간에 스왑가능
* 레디큐: 메모리 이미지가 뒤에 저장되있거나 메모리에있는 모든 프로세스, 디스패커가 스왑아웃하거나 다시 불러온다.

### 스와핑을 포함한 컨텍스트 스위칭 시간

* 길다.
* 스왑시간의 대부분은 전송 시간이다.
* 스와핑 제약조건
  * 임박한 I/O: I/O이 끝나기를 기다리는 프로세스는 스왑아웃하면 안된다. I/O를 다하면 다른 프로세스 메모리가 필요할수도있음
  * 커널 버퍼안에서 수행하는 I/O 더블 버퍼링 오버헤드

### 연속 메모리 할당

* 메인 메모리는 OS와 유저 프로세스 모두 지원한다.
* 모두 할당시키는 방법중에 한개가 연속 메모리 할당이다.
* 메모리 파티션
  * OS는 낮은 메모리에 유저는 높은 메모리에 저장된다.
  * 각 프로세스는 한개의 연속적인 섹션에 있다.
* 메모리 보호
  * 베이스 레지스터는 가장 작은 물리주소를
  * 리밑 레지스터는 범위를
  * MMU는 논리 주소를 동적으로 매핑한다.
  * 레지스터 스킴을 제배치하는것이 OS 크기를 동적으로 바뀌게 한다.

### 메모리 할당

* 메모리 파티션 할당
  * 고정된 메모리 파티션: 각 파티션에는 하나의 프로세스가 있다. 배치 환경에서 자주쓴다.
  * 변동 메모리 파티션: 구멍: 프리, OS는 빈공간 찾음
  * first fit: 첫 공간에다가
  * best: 가장 작은 적합한 구멍
  * 최악: 가장 큰

### fragmentation

* 외부: 큰공간이 있는데 연속적이지 않다.
* 내부: 큰 공간을 할당받음, 빈공간이 생김
* first fit은 절반정도 못씀 50%룰
* 외부 단편화 줄임
  * 컴팩션: 메모리 내용물을 섞고 합친다.
    * 동적할당과 런타임 할당에서만 가능

### 세그멘테이션

* 세그먼트
  * 다양한길이
  * 세그먼트안에 있는 요소는 오프셋에 의해서 식별된다.
* 세그멘테이션
  * 세크먼트의 적용을 도와주는 메모리 관리 스키마다.
  * 논리적 주소는 세그먼트의 모음이다.
  * 각 세그먼트는 이름과 길이가 있다.
  * 주소는 번호,오프셋
* 세그먼트 테이블, 세그먼트 베이스, 세그먼트 리미트

### 페이징

* 물리주소를 불연속하게 하는것을 허용하는 메모리 관리 스키마다.(외부 단편화 회피)
* 기본 개념
  * 물리 메모리를 프레임이라고 불리는 고정된 사이즈로 나눈다.
  * 논리적인 메모리를 페이지라고 불리는 고정된 사이즈로 나눈다.
* 페이지 넘버와 페이지 오프셋으로 주소 구성
* 페이지 사이즈는 2의 거듭제곱

### 페이징에서 단편화

* 외부 단편화는 없다.
* 내부 단편화
  * 딱 맞게 나눠지지않는다.
  * 작은 페이지 사이즈는 더 큰 페이지 테이블 오버헤드
  * 큰 페이지 사이즈는 디스크 I/O에 도움이 된다. 헤더를 적게 움직이고 많은 것을 읽을 수 있어서
* 4KB에서 8KB가 전형적임

### 페이지에 관여하는 하드웨어

* 다양한 OS는 페이지 테이블을 가진다.
  * 지정된 레지스터 사용
    * 페이지 주소 변환을 위한 빠른 회로
    * 컨텍스트 스위치에 따라 디스패커가 리로드한다.
    * 작은 크기의 페이지표만 가능
  * 프로세스 마다 메모리에 페이지 테이블을 가진다.
    * PCB에 페이지 데이블 포인터를 넣는다.
    * 레지스터는 루트 페이지를 가르킨다.
      * page-table base register(PTBR): 여러번 메모리에 접속해야함

### TLB

* Translation look-aside buffer
  * 특별한 작고 빠른 검색 하드웨어이다.
  * 키와 값을 가지고있다.
  * 32에서 1024개의 엔트리를 가짐
  * 빠르다.
* TLB 미스: 페이지 넘버가 TLB에 없었음
  * TLB에 새로 넣는다.
  * 이미 꽉차있으면 오랜된거 한개 정책에 맞게 교환함
* TLB는 컨텍스트 스위칭하면 지워진다.
* ASIDs(address-space identifiers)
  * 프로세스간에 보호를 제공함
  * ASIDs가 없으면 무조건 지워야한다.
* hit ratio: 얼마나 페이지 넘버 찾는가?

### 페이지 보호

* 보호 비트
  * read-write/read-only
  * valid/invalid: valid면 페이지가 프로세스의 공간에 들은것
* PTLR: page table length register

### 공유페이지

* 페이지 테이블을 사용하여 페이지를 공유
  * 프로세스는 코드(reentrant code) 페이지를 공유할수있다.
  * 한개만 메모리에 존재

### 페이지 테이블 구조

* 하어라키 페이징(32비트)
  * 큰 논리적인 주소 공간 -> 큰 페이지 테이블
  * 페이지 사이즈 4K면 페이지 테이블 2^20
* 해쉬 페이지 테이블
  * 32비트 이상
  * 가상 페이지 번호가 해쉬테이블에 해쉬된다.
  * 가상 페이지 번호는 연결 리스트에 요소와 비교된다.
* 인벌티드 페이지 테이블
  * 페이지 테이블은 각 프로세스에 존재한다. 많은 프로세스가 있음 테이블 많이 필요-> 공간 낭비
  * 하나의 엔트리가 메모리의 프레임을 가르킨다.
  * 느린 속도-> 해쉬 사용가능
  * 공유 페이지 구현이 어렵다.

## 가상 메모리 관리

* 가상 메모리 관리
  * 메모리에 완전히 없는 프로세스를 돌리는 방법
    * 메모리보다 클수있다.
    * 큰 동일한 공간으로 메인 메모리를 추상화함
* 인스트럭션은 실행을 하려면 메인 메모리에 있어야한다.
  * 모든 프로그램이 필요한거는아니다.
  * 동시에 있을필요는 없는거
* 부분적으로 로드하면 장점
  * 물리적 메모리의 양에 제약이 없다.
  * 동시에 더 많은 프로그램을 돌릴수있다.
  * 유저 프로그램 교체하는데 덜 시간을 사용해도됨

### 페이지 요구

* 페이지는 필요할때 로드된다.
* 기본 개념
  * 스왑인할때 페이저는 어떤 페이지가 사용될지 추측하고 가져온다
  * 로드된 페이지와 디스크에 있는 페이지의 구분이 필요 valid-invalid bit로 가능

### 페이지 부재

* 페이지 부재는 프로세스가 메인 메모리에 없는 페이지에 접근할때 발생한다.
* 극한의 상황
  * 메모리에 페이지가 없는 프로그램을 시작함
* 페이지 부재에 있는 인스트럭션 재시작
  * 인스트럭션이 여러번 메모리에 접근할수도있다.
  * 페이지 부재는 언제나 발생할수있다.
  * 페이지 부재로 다시 모든 과정을 해야할수도 있다.

### Copy on write(COW)

* fork 시스템콜
  * 자식 프로세스는 부모를 복사한다.
  * exec이 바로 호출되면 모든 페이지를 복사하는것을 낭비다.
* COW
  * 초기에 자식과 부모는 같은 페이지를 공유한다.
  * 특정 페이지를 수정할때 새 페이지를 복사하고 수정한다.
* vfork(): 페이지 복제 없이, 자식이 exec 호출할때까지 유예한다.

### 페이지 교체

* 페이지 초과할당
  * 사용중인 페이지의 부분을 유지한다.
  * 갑자기 모든 페이지를 원하면 우짜냐?
* 한개 내보내고 한개 넣는다.
* 이런 오버해드 어떻게 줄일까? modify bit

### FIFO

* 오래된거 내보내, Belady’s anomaly(페이지 프레임을 늘리면 줄어야되는데 오히려 늘어남)

### Optimal

* 오랫동안 안쓸것같은거 내보냄
* 구현이 어렵다

### LRU(least recently used)

* optimal에 근접함
* 과거를 보고 미래를 예측
  * 가장 오랬동안 사용안된거 배출
* 카운터
  * CPU에 시계 레지스터 추가, 메모리 접근 할때마다 1식 증가
  * 페이지 레퍼런스에 그 시간 값도 복사된다.
  * 가장 작은 값을 가진 친구 교체
* 스택
  * 페이지 넘버 스택을 유지
  * 페이지가 언급되면 스택에서 제거하고 다시 넣음

### LRU Approximation

* 하드웨어 지원이 없으면?
* 레퍼런스 비트가 있다.
  * 하드웨어에 의해 어세스 될때마다 셋됨
  * 초기에는 0이다
  * 어느 페이지가 접근됬는지는 알수있다
* Additional reference bits 알고리즘
  * 주기적으로 레퍼런스 비트에 순서를 맞춘다.
  * 페이지의 8비트를 타이머마다 오른쪽으로 쉬프트한다.
  * 가장 작은 숫자가 LRU이다.
* second chance 알고리즘
  * 0 비트를 가진 페이지를 선택한다.
  * 만약에 레퍼런스 비트가 1이면 그것을 0으로 하고 어세스 시간을 현제 시간으로 초기화한다.
  * 페이지가 활성화 되있다면 비트는 항상 켜져있고 페이지는 교체 되지 않을것이다
* enhanced seconde chance
  * 레퍼런스 비트하고 모디파이 비트도 고려하자

### LFU

* 가장 적은 숫자가 교체됨

### MFU

* 근거: 적은 수자가 최근에 가져왔음

### thrasing

* 프로세스에 프레임을 할당
  * 프레임의 최소 수의 개념
  * 동등 또는 비례하는 할당?
  * 전역 교체 부분 교체
* 만약 프로세스가 충분한 페이지가 없다면 페이지 폴트가 급격하게 늘것임
* 원인
  * 멀티프로그램의 증가
  * 한 프로그램이 더 많은 프레임이 필요해지고 전역으로 교체된다.
  * 다른 프로그램도 또한 프레임이 필요하고 페이지 폴트가 일어남
  * 페이지 폴트 증가로 CPU 활성도가 감소
  * 활성도 증가로 더 많은 멀티 프로그래밍을 함

### 워킹셋 모델

* 스레싱을 줄이기 위해 프레임의 특정번호를 줘야한다.
  * 얼마나 많은 프레임이 필요한가?
* 워킹셋은 지역성을 기반으로 가장 많이 쓴는 페이지를 메모리에 계속 상주시킨다.
